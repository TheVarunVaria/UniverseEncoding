{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UniverseEncoding","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN4XogCoDR4MoNTN/4TBcoY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1q2irVUHO7sr","colab_type":"text"},"source":["# this version has minimal modifications by wz\n","\n","original: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb "]},{"cell_type":"markdown","metadata":{"id":"ExHQCOCGPJp0","colab_type":"text"},"source":["##### Copyright 2018 The TensorFlow Hub Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");"]},{"cell_type":"markdown","metadata":{"id":"4PVMgAz9Pzcr","colab_type":"text"},"source":["# Universal Sentence Encoder\n","\n","\n","<table align=\"left\"><td>\n","  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n","  </a>\n","</td><td>\n","  <a target=\"_blank\"  href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\">\n","    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","</td></table>"]},{"cell_type":"markdown","metadata":{"id":"nND5J04uN6sm","colab_type":"text"},"source":["More detailed information about installing Tensorflow can be found at \n","[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)"]},{"cell_type":"markdown","metadata":{"id":"N4uVceNxP-nO","colab_type":"text"},"source":["This notebook illustrates how to access the Universal Sentence Encoder and use it for sentence similarity and sentence classification tasks.\n","\n","The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data."]},{"cell_type":"markdown","metadata":{"id":"qYzJ-8DTPUoh","colab_type":"text"},"source":["# 1. Importing the libraries"]},{"cell_type":"code","metadata":{"id":"WErRr_uhk4c4","colab_type":"code","outputId":"3ef89a2c-0a1b-4443-e3c0-87169cdfd60d","executionInfo":{"status":"ok","timestamp":1587001251861,"user_tz":300,"elapsed":1227,"user":{"displayName":"Varun Ketan Varia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKHlV33ShWTnWMaaY8-As0pcq0f-kvYtpuC3IlwA=s64","userId":"01230964141385336374"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["%tensorflow_version 1.6\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import re\n","import seaborn as sns"],"execution_count":3,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.6`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vIM1VdbaQYL3","colab_type":"text"},"source":["# 2. Set the Module url"]},{"cell_type":"code","metadata":{"id":"Pl0h0dofk9dA","colab_type":"code","colab":{}},"source":["#Set the moddule URL\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n","# set embed variable to set module url\n","embed = hub.Module(module_url)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_FzFJqUXG7q","colab_type":"code","outputId":"e2bec628-a9e1-4f31-9315-b41d748aa205","executionInfo":{"status":"ok","timestamp":1587001006613,"user_tz":300,"elapsed":20135,"user":{"displayName":"Varun Ketan Varia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKHlV33ShWTnWMaaY8-As0pcq0f-kvYtpuC3IlwA=s64","userId":"01230964141385336374"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#download the model to local so it can be used again and again\n","#os.mkdir('../module_useT')\n","#os.mkdir('../modulke')\n","# Download the module, and uncompress it to the destination folder. \n","!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ../module_useT"],"execution_count":12,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","./\n","./tfhub_module.pb\n","./variables/\n","./variables/variables.data-00000-of-00001\n"," 94  745M   94  704M    0     0  45.5M      0  0:00:16  0:00:15  0:00:01 63.4M./variables/variables.index\n","./assets/\n","./saved_model.pb\n","100  745M  100  745M    0     0  45.8M      0  0:00:16  0:00:16 --:--:-- 60.2M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SM7vSZjeYK-X","colab_type":"code","colab":{}},"source":["#os.getcwd()\n","directory"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XVeDxiDPOJe","colab_type":"text"},"source":["# 3. Connecting to the google drive"]},{"cell_type":"code","metadata":{"id":"aUp6JhrGa9ad","colab_type":"code","colab":{}},"source":["#Connect to the Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJzPKbK5PjJG","colab_type":"text"},"source":["# Create and Change the directory"]},{"cell_type":"code","metadata":{"id":"TSdXz_WfjZD0","colab_type":"code","colab":{}},"source":["#Unzip the folder and create a new folder for unzipped files\n","!mkdir wiki_archived  #create a directory named test/\n","!unzip -q Archive.zip -d wiki_archived/  #unzip data in train/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQxaCfAbTAgt","colab_type":"code","colab":{}},"source":["os.chdir('/content/gdrive/My Drive/Colab Notebooks')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJ-RQb55P2TC","colab_type":"text"},"source":["# Changing the directory to new created directory"]},{"cell_type":"code","metadata":{"id":"devoTKnAqA03","colab_type":"code","colab":{}},"source":["# Change and set directory to access the unzipped files\n","directory = '/content/gdrive/My Drive/Colab Notebooks/wiki_archived'\n","os.chdir(directory)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKGgeJ4Lnk8d","colab_type":"code","colab":{}},"source":["directory = '/content/gdrive/My Drive/Colab Notebooks/wiki_archived'\n","os.chdir(directory)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0-Hi2iAjYPs","colab_type":"code","colab":{}},"source":["# View the contents of the folder\n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6OPcXqivQBFE","colab_type":"text"},"source":["# Iterate through each file that ends in 'xml'"]},{"cell_type":"code","metadata":{"id":"gY1iUIsRq5Wv","colab_type":"code","colab":{}},"source":["# Access all the files from the folder for files with 'xml' ending\n","count = 0\n","#For reading mulitple files and appending to paragraph[]\n","paragraph = []\n","para = []\n","for filename in os.listdir(directory):\n","  #for i in range(2):\n","\n","  if filename.endswith(\".xml\"):    \n","    with open(filename,'r') as fi:\n","      para = []\n","      count = count + 1\n","      print(count)\n","      para.append(count)\n","print(para)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BnvGdsXNQICc","colab_type":"text"},"source":["# Read a specific file (Hardcode the file name)"]},{"cell_type":"code","metadata":{"id":"QH08eMRDk17h","colab_type":"code","colab":{}},"source":["#Reading a single given text file and appending it to paragraph[]\n","#fn = \"20121202-wiki-en_000002.txt.xml\"\n","#fi = open(\"20121202-wiki-en_000002.txt.xml\", 'r')\n","#text = fi.read()\n","\n","paragraph=[]\n","with open(\"20121202-wiki-en_000007.txt.xml\",'r') as fi:\n","  for line in fi:\n","    paragraph.append(line)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w68XrtpnkTHj","colab_type":"code","colab":{}},"source":["paragraph=[]\n","with open(\"20121202-wiki-en_000007.txt.xml\",'r') as fi:\n","  for line in fi:\n","    paragraph.append(line)\n","messages = paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qsBf4V_l-BD","colab_type":"code","colab":{}},"source":["# Compute a representation for each message, showing various lengths supported.\n","word = \"Elephant\"\n","sentence = \"I am a sentence for which I would like to get its embedding.\"\n","paragraph = (\n","    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n","    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n","    \"the more 'diluted' the embedding will be.\")\n","messages = [word, sentence, paragraph]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sb-cNK3k2G3","colab_type":"code","colab":{}},"source":["messages = \"elephant\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dh70JlDTNJuY","colab_type":"text"},"source":["# Final code to write and read 2000 documents"]},{"cell_type":"markdown","metadata":{"id":"W8vWKvQVwxe4","colab_type":"text"},"source":["Test\n"]},{"cell_type":"code","metadata":{"id":"TcwO7L9AwwCP","colab_type":"code","colab":{}},"source":["####\n","import time\n","import re\n","count = 0\n","with tf.Session() as session:\n","  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","#Reading a single given text file and appending it to paragraph[]\n","  for filename in os.listdir(directory):\n","    if filename.endswith(\".xml\"):    \n","      with open(filename,'r') as fi:\n","        paragraph=[]\n","    #with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","        for line in fi:\n","          match = re.search(r'<DOC',line)\n","      #if this is the start of the document, reset the contents of paragraph\n","          if match:\n","            count = count + 1\n","            print (count)\n","          paragraph.append(line)\n","\n","          messages = paragraph\n","\n","          #start_time = time.time()\n","          message_embeddings = session.run(embed(messages))\n","\n","          with open('Encoding'+str(fi)+'.txt', 'w+') as filehandle:\n","            for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","              filehandle.write(\"Message: {}\".format(messages[i]))\n","              filehandle.write(\"Embedding size: {}\".format(len(message_embedding)))\n","              message_embedding_snippet = \", \".join(\n","                  (str(x) for x in message_embedding[:3]))\n","              filehandle.write(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n","\n","          #end_time = time.time()\n","        #print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qP7Vyy-EWeeh","colab_type":"code","outputId":"5ae1dde0-141e-48e4-af98-c4b5db43b771","executionInfo":{"status":"error","timestamp":1587001234041,"user_tz":300,"elapsed":3069,"user":{"displayName":"Varun Ketan Varia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKHlV33ShWTnWMaaY8-As0pcq0f-kvYtpuC3IlwA=s64","userId":"01230964141385336374"}},"colab":{"base_uri":"https://localhost:8080/","height":351}},"source":["# Initialising the session once\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","def embed_useT(module):\n","    with tf.Graph().as_default():\n","        sentences = tf.placeholder(tf.string)\n","        embed = hub.Module(module)\n","        embeddings = embed(sentences)\n","        session = tf.train.MonitoredSession()\n","    return lambda x: session.run(embeddings, {sentences: x})\n","embed_fn = embed_useT('../module_useT')\n","\n","embed_fn(messages)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4ebed3fd431c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitoredSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0membed_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_useT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../module_useT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0membed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-4ebed3fd431c>\u001b[0m in \u001b[0;36membed_useT\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_useT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"]}]},{"cell_type":"code","metadata":{"id":"3hQF_DUHoi5W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3bcafa5-da4a-4069-e102-9a2bb0990411","executionInfo":{"status":"error","timestamp":1587001112662,"user_tz":300,"elapsed":1701,"user":{"displayName":"Varun Ketan Varia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKHlV33ShWTnWMaaY8-As0pcq0f-kvYtpuC3IlwA=s64","userId":"01230964141385336374"}}},"source":["with open('Sample_rest.txt', 'w+') as filehandle:\n","  filehandle.write(embed_fn(messages))"],"execution_count":14,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-9a79ed48bb23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample_rest.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mfilehandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not numpy.ndarray"]}]},{"cell_type":"code","metadata":{"id":"T1iMC9jYka6T","colab_type":"code","outputId":"ff41736a-1427-4e8a-ed1a-ff7ae4ebdd3c","executionInfo":{"status":"error","timestamp":1587001220777,"user_tz":300,"elapsed":1467,"user":{"displayName":"Varun Ketan Varia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKHlV33ShWTnWMaaY8-As0pcq0f-kvYtpuC3IlwA=s64","userId":"01230964141385336374"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["embed_fn(messages)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-37784219fe82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'embed_fn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"eLDvbi2UepCV","colab_type":"code","colab":{}},"source":["## 04/08/2020\n","#import time\n","#count = 0\n","#paragraph = []\n","embed_fn = embed_useT('../module_useT')\n","filename = \"20121202-wiki-en_000007.txt.xml\"\n","#for filename in os.listdir(directory):\n","if filename.endswith(\".xml\"):\n","  with open(filename,'r') as fi:\n","    paragraph=[]\n","    for line in fi:\n","      paragraph.append(line)\n","    messages = paragraph\n","  #print(\"hello\")\n","  print(embed_fn(messages))\n","  #print(\"hel\")\n","    #with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","      #count = count + 1\n","      #print (count)\n","    #with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","    #start_time = time.time()\n","    #for line in fi:\n","      #paragraph.append(line)\n","        #for line in fi:\n","          #match = re.search(r'<DOC',line)\n","      #if this is the start of the document, reset the contents of paragraph\n","          #if match:\n","      #paragraph.append(line)\n","      #messages = paragraph\n","          \n","      \n","      #embed_fn(messages)\n","#end_time = time.time()\n","#print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xhfa6qxBik1x","colab_type":"code","colab":{}},"source":["1+1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vD_7aCnEfirF","colab_type":"code","colab":{}},"source":["paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z20ymunmbrRf","colab_type":"code","colab":{}},"source":["## 04/08/2020\n","def embed_useT(module):\n","  with tf.Graph().as_default():\n","      sentences = tf.placeholder(tf.string)\n","      embed = hub.Module(module)\n","      embeddings = embed(sentences)\n","      session = tf.train.MonitoredSession()\n","  return lambda x: session.run(embeddings, {sentences: x})\n","count = 0\n","for filename in os.listdir(directory):\n","  \n","    if filename.endswith(\".xml\"):    \n","      with open(filename,'r') as fi:\n","        paragraph=[]\n","    #with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","        for line in fi:\n","          match = re.search(r'<DOC',line)\n","      #if this is the start of the document, reset the contents of paragraph\n","          if match:\n","            count = count + 1\n","            print (count)\n","          paragraph.append(line)\n","\n","          messages = paragraph\n","          \n","          embed_fn = embed_useT('../module_useT')\n","          embed_fn(messages)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TH5uOWyfX36I","colab_type":"code","colab":{}},"source":["messages\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gtOt2GAcRjO","colab_type":"code","colab":{}},"source":["paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6QinIoMLJim","colab_type":"code","colab":{}},"source":["####\n","import time\n","import re\n","count = 0\n","#Reading a single given text file and appending it to paragraph[]\n","for filename in os.listdir(directory):\n","  if filename.endswith(\".xml\"):    \n","    with open(filename,'r') as fi:\n","      paragraph=[]\n","  #with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","      for line in fi:\n","        match = re.search(r'<DOC',line)\n","    #if this is the start of the document, reset the contents of paragraph\n","        if match:\n","          count = count + 1\n","print (count)\n","\n","with open('Encoding'+str(count)+'.txt', 'w+') as filehandle:\n","  filehandle.write(\"Message:\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9ScEGCV1GnZ","colab_type":"code","colab":{}},"source":["#### Almost there\n","\n","#Reading a single given text file and appending it to paragraph[]\n","import time\n","count = 0\n","for filename in os.listdir(directory):\n","  #for i in range(2):\n","  if filename.endswith(\".xml\"):\n","    with open(filename,'r') as fi:\n","      paragraph=[]\n","      count = count + 1\n","      print(count)\n","      for line in fi:\n","        paragraph.append(line)\n","        messages = paragraph\n","        \n","        start_time = time.time()\n","\n","        with tf.Session() as session:\n","          session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","          message_embeddings = session.run(embed(messages))\n","\n","\n","          with open('Answer' + filename+'.txt', 'w+') as filehandle:\n","            for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","              filehandle.write(\"Message: {}\".format(messages[i]))\n","              filehandle.write(\"Embedding size: {}\".format(len(message_embedding)))\n","              message_embedding_snippet = \", \".join(\n","                  (str(x) for x in message_embedding[:3]))\n","              filehandle.write(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n","\n","        end_time = time.time()\n","        print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hg9vVVm4yrb9","colab_type":"code","colab":{}},"source":["with open('000002' + 'Answer.txt', 'w+') as filehandle:\n","  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    filehandle.write(\"Message: {}\".format(messages000002[i]))\n","    filehandle.write(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    filehandle.write(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lg14FZ5xiU1v","colab_type":"code","colab":{}},"source":["paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"25G3Ew9MyFgp","colab_type":"code","colab":{}},"source":["# set messages to paragraph since messages is passed to the encoder\n","messages = paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKtBbz_NUKNl","colab_type":"text"},"source":["# Encoder function"]},{"cell_type":"markdown","metadata":{"id":"87NGioD9RqHQ","colab_type":"text"},"source":["## a. Encode function without printing the embeddings\n","\n","Execute this when you just want to view the time taken for each document"]},{"cell_type":"code","metadata":{"id":"tk4uwvyWyIO7","colab_type":"code","colab":{}},"source":["#Encoder function\n","\n","import time\n","start_time = time.time()\n","\n","with tf.Session() as session:\n","  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","  message_embeddings = session.run(embed(messages))\n","\n","  \"\"\"for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    print(\"Message: {}\".format(messages[i]))\n","    print(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\"\"\"\n","\n","\n","end_time = time.time()\n","print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4D3gNz9iRuxc","colab_type":"text"},"source":["## b. Encoder function that prints the embeddings"]},{"cell_type":"code","metadata":{"id":"HylUrpjwl5qs","colab_type":"code","colab":{}},"source":[" #Encoder function\n","\n","import time\n","start_time = time.time()\n","\n","with tf.Session() as session:\n","  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","  message_embeddings = session.run(embed(messages))\n","\n","  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    print(\"Message: {}\".format(messages[i]))\n","    print(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n","\n","\n","end_time = time.time()\n","print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69TR-qZpS_lY","colab_type":"text"},"source":["# Defining a re-usable Encoder funtion that takes messages as input and calculates the embeddings\n","\n","<li>Function name: encoder </li>\n","<li>Parameters : Takes one array parameter</li>"]},{"cell_type":"code","metadata":{"id":"b6m_ssHdGRKy","colab_type":"code","colab":{}},"source":["def encoder (messages):\n","  import time\n","  start_time = time.time()\n","\n","\n","  message_embeddings = session.run(embed(messages))\n","\n","\n","\n","  \"\"\"for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    print(\"Message: {}\".format(messages[i]))\n","    print(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\"\"\"\n","\n","  end_time = time.time()\n","  return (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpFs_d4EHDNc","colab_type":"code","colab":{}},"source":["session =  tf.Session()\n","session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","print(encoder(messages))\n","session.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnXS4jAFTKor","colab_type":"text"},"source":["# Using regular expression to separate files"]},{"cell_type":"markdown","metadata":{"id":"H4YdXh72Tm6o","colab_type":"text"},"source":["## initializing the session before opening the file\n","I am not sure if this is the correct way to do it.\n",">to do: cross verify the timing and the approach"]},{"cell_type":"code","metadata":{"id":"tkYHVbiVYnk8","colab_type":"code","colab":{}},"source":["# Using regex for extract a separate document\n","fi = open(\"20121202-wiki-en_000002.txt.xml\", 'r')\n","#text = fi.read()\n","count = 0\n","paragraph=[]\n","session =  tf.Session()\n","session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","  for line in fi:\n","    match = re.search(r'<DOC',line)\n","    #if this is the start of the document, reset the contents of paragraph\n","    if match:\n","      if(count>0):\n","\n","        messages = paragraph\n","        print(count)\n","        message_embeddings = session.run(embed(messages))\n","        print(message_embeddings)\n","      paragraph = []\n","      count= count + 1\n","    #Append the line to paragraph\n","    #The purpose of this code will be make sure paragraph will contain the contents of\n","    # a single doc file\n","    paragraph.append(line)\n","    #Call the encoder function\n","session.close()\n","\n","\"\"\"with tf.Session() as session:\n","  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","  message_embeddings = session.run(embed(messages))\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3imiPj4ATwlC","colab_type":"text"},"source":["## Initializing the session after opening the file. Similar to the previous code.\n","> Need to cross verify if this is efficient."]},{"cell_type":"code","metadata":{"id":"LyjfrpoInGAK","colab_type":"code","colab":{}},"source":["# Using regex for extract a separate document\n","fi = open(\"20121202-wiki-en_000002.txt.xml\", 'r')\n","text = fi.read()\n","count = 0\n","paragraph=[]\n","#session =  tf.Session()\n","#session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","  for line in fi:\n","    match = re.search(r'<DOC',line)\n","    #if this is the start of the document, reset the contents of paragraph\n","    if match:\n","      if(count>0):\n","\n","        messages = paragraph\n","        print(count)\n","        print(messages)\n","      paragraph = []\n","      count= count + 1\n","    #Append the line to paragraph\n","    #The purpose of this code will be make sure paragraph will contain the contents of\n","    # a single doc file\n","    paragraph.append(line)\n","    #Call the encoder function\n","#session.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmPKCBArT-Ra","colab_type":"text"},"source":["# Using regex for a specific file"]},{"cell_type":"code","metadata":{"id":"2uOelXgsLu0C","colab_type":"code","colab":{}},"source":["# Using regex for extract a separate document\n","import re\n","fi = open(\"20121202-wiki-en_000002.txt.xml\", 'r')\n","text = fi.read()\n","count = 0\n","paragraph2=[]\n","with open(\"20121202-wiki-en_000002.txt.xml\",'r') as fi:\n","  for line in fi:\n","    match = re.search(r'<DOC',line)\n","    if match:\n","      '''if(count>0):\n","        messages = paragraph\n","        encoder(messages)'''\n","      paragraph2 = []\n","      count= count + 1\n","    paragraph2.append(line)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsuJpUudRZpp","colab_type":"text"},"source":["# Work in Progress"]},{"cell_type":"code","metadata":{"id":"f6wyuCxn6OlD","colab_type":"code","colab":{}},"source":["paragraph2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_vBaKY5fl6L","colab_type":"code","colab":{}},"source":["count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEFmCqYDliAP","colab_type":"code","colab":{}},"source":["text = ['<DOC>',\n"," '<title>Alexis Carrel</title>',\n"," '<docno>wikipedia245</docno>',\n"," '<text> Carrel in 1912 Alexis Carrel (June 28, 1873 &ndash; November 5, 1944) was a French surgeon and biologist who was awarded the Nobel Prize in Physiology or Medicine in 1912 for pioneering vascular suturing techniques. He invented the first perfusion pump with Charles A. Lindbergh opening the way to organ transplantation. Like many intellectuals before World War II he promoted eugenics. He was a regent for the French Foundation for the Study of Human Problems during the Nazi occupation of Vichy France which implemented the eugenics policies there; his association with the Foundation led to allegations of collaborating with the Nazis. Reggiani, Andrés Horacio. God\\'s Eugenicist. Alexis Carrel and the Sociobiology of Decline. Berghahn Books, Oxford 2007. Schneider William H.. Quality and Quantity: The Quest for Biological Regeneration in Twentieth-Century France. Cambridge UP 1990, pp. 272-282. (see Andrés Horacio Reggiani, Alexis Carrel, the Unknown: Eugenics and Population Research under Vichy, as well as Caillois, p. 107)  == Biography == Born in Sainte-Foy-lès-Lyon, Rhône, Carrel was raised in a devout Catholic family and was educated by Jesuits, though he had become an agnostic by the time he became a university student. Jaki He received his medical degree from Université de Lyon, and practiced in France and in the United States at the University of Chicago and the Rockefeller Institute for Medical Research. He developed new techniques in vascular sutures and was a pioneer in transplantology and thoracic surgery. Alexis Carrel was also a member of learned societies in the U.S., Spain, Russia, Sweden, the Netherlands, Belgium, France, Vatican City, Germany, Italy and Greece and received honorary doctorates from Queen\\'s University of Belfast, Princeton University, California, New York, Brown University and Columbia University.  In 1902 he witnessed the miraculous cure of Marie Bailly at Lourdes, made famous in part because she named Carrel as a witness of her cure. After the fame surrounding the event, Carrel could not obtain a hospital appointment because of the pervasive anticlericalism in the French university system at the time. In 1903 he emigrated to Montreal, Canada, but soon relocated to Chicago, Illinois to work for Hull Laboratory. While there he collaborated with American physician Charles Claude Guthrie in work on vascular suture and the transplantation of blood vessels and organs as well as the head, and Carrel was awarded the 1912 Nobel Prize in Physiology or Medicine for these efforts. Nobelprize.org  In 1906 he joined the newly-formed Rockefeller Institute of Medical Research in New York where he spent the rest of his career. Reggiani In the 1930s, Carrel and Charles Lindbergh became close friends not only because of the years they worked together but also because they shared personal, political, and social views. Lindbergh initially sought out Carrel to see if his sister-in-law\\'s heart, damaged by rheumatic fever, could be repaired. When Lindburgh saw the crudeness of Carrel\\'s machinery, he offered to build new equipment for the scientist. Eventually they built the first perfusion pump, an invention instrumental to the development of organ transplantation and open heart surgery. Lindbergh considered Carrel his closest friend, and said he would preserve and promote Carrel\\'s ideals after his death.  Due to his close proximity with Jacques Doriot\\'s fascist Parti Populaire Français (PPF) during the 1930s and his role in implementing eugenics policies during Vichy France, he was accused after the Liberation of collaborationism, but died before the trial. Carrel spent his life promoting spiritualism, though he did not embrace the Catholicism of his youth. In 1939 he met with Trappist monk Alexis Presse on a recommendation. Though Carrel was skeptical about meeting with a priest Presse ended up having a profound influence on the rest of Carrel\\'s life. He summoned Presse to administer the Catholic Sacraments on his death bed in November 1944.  == Contributions to science == === Vascular suture === Carrel was a young surgeon in 1894 when the French president Sadi Carnot was assassinated with a knife. His large abdominal veins had been severed, and surgeons who treated the president felt that such veins were too large to be successfully reconnected. This left a deep impression on Carrel, and he set about developing new techniques for suturing blood vessels. The technique of \"triangulation\", which was inspired by sewing lessons he took from an embroideress, is still used today. Julius Comroe wrote: \"Between 1901 and 1910, Alexis Carrel, using experimental animals, performed every feat and developed every technique known to vascular surgery today.\" He had great success in reconnecting arteries and veins, and performing surgical grafts, and this led to his Nobel Prize in 1912.  === Wound antisepsis === During World War I (1914–1918), Carrel and the English chemist Henry Drysdale Dakin developed the Carrel-Dakin method of treating wounds based on chlorine (Dakin\\'s solution) which, preceding the development of antibiotics, was a major medical advance in the care of traumatic wounds. For this, Carrel was awarded the Légion d\\'honneur. === Organ transplants === Carrel co-authored a book with famed pilot Charles A. Lindbergh, The Culture of Organs, and worked with Lindbergh in the mid-1930s to create the \"perfusion pump,\" which allowed living organs to exist outside of the body during surgery. The advance is said to have been a crucial step in the development of open-heart surgery and organ transplants, and to have laid the groundwork for the artificial heart, which became a reality decades later. Red Gold . Innovators & Pioneers . Alexis Carrel | PBS Some critics of Lindbergh claimed that Carrel overstated Lindbergh\\'s role to gain media attention, (Wallace, American Axis p. 101) but other sources say Lindbergh played an important role in developing the device. The Doric Column - Lindbergh & Carrel, organ perfusion, tissue culture, transplants, gene therapy The \"Lone Eagle\\'s\" Contribution to Cardiology Both Lindbergh and Carrel appeared on the cover of Time magazine on June 13, 1938. === Cellular senescence === Carrel was also interested in the phenomenon of senescence, or aging. He claimed incorrectly that all cells continued to grow indefinitely, and this became a dominant view in the early 20th century. ; page 24. Carrel started an experiment on January 17, 1912 where he placed tissue cultured from an embryonic chicken heart in a stoppered Pyrex flask of his own design. He maintained the living culture for over 20 years with regular supplies of nutrient. This was longer than a chicken\\'s normal lifespan. The experiment, which was conducted at the Rockefeller Institute for Medical Research, attracted considerable popular and scientific attention. Carrel\\'s experiment was never successfully replicated, and in the 1960s Leonard Hayflick and Paul Moorhead proposed that differentiated cells can only undergo a limited number of divisions before dying. This is known as the Hayflick limit, and is now a pillar of biology.  It is not certain how Carrel obtained his anomalous results. Leonard Hayflick suggests that the daily feeding of nutrient was continually introducing new living cells to the alleged immortal culture. J. A. Witkowski has argued that, while \"immortal\" strains of visibly mutated cells have been obtained by other experimenters, a more likely explanation is deliberate introduction of new cells into the culture, possibly without Carrel\\'s knowledge. Witkowsky explanation is actually based on the account of a visiting medical researcher, Ralph Buchbaum, who reports being told by a technician in Carrel\\'s lab \"Dr. Carrel would be so upset if we lost the strain, we just add a few embryo cells now and then\". After the first six months, Carrel\\'s colleague Albert Ebeling had actually taken charge of the cultures and published several papers about their development, until they were eventually discarded in 1946. Witkowsky, in \"Dr. Carrel\\'s immortal cells\", op. cit., quotes Buchbaum\\'s account. At the end Buchbaum writes that \"I told this story, of my visit to Carrel\\'s laboratory, to various people. Dr. Bloom (Buchbaum\\'s director of research in Chicago)refused to believe it. Others chuckled gleefully. Dr. Carrel was to blame only in that he did not keep on top of what was really going on in the laboratory (mostly, he wrote the papers). Dr. Parker and Dr. Ebeling probably suspected something, hence the \"retirement\". In the interest of truth and science, the incident should have been thoroughly investigated. If it had been, some heads might have rolled, sacrificed to devotion to a wrong hypothesis - immortality of cell strains.\". Witkowsky also reports a Dr. Margaret Murray telling him that \"one of Carrel\\'s technicians of that time was passionately anti-fascist and detested Carrel\\'s political and social ideas\" and expressing her belief that \"that this technician would willingly have discredited Carrel scientifically if possible.\".  === Honors === In 1972, the Swedish Post Office honored Carrel with a stamp that was part of its Nobel stamp series. The Nobel Stamps of 1972 In 1979, the lunar crater Carrel was named after him as a tribute to his scientific breakthroughs. In February 2002, as part of celebrations of the 100th anniversary of Charles Lindbergh\\'s birth, the Medical University of South Carolina at Charleston established the Lindbergh-Carrel Prize, Charles Lindbergh Symposium given to major contributors to \"development of perfusion and bioreactor technologies for organ preservation and growth\". Michael DeBakey and nine other scientists Charles Lindbergh Symposium received the prize, a bronze statuette created for the event by the Italian artist C. Zoli and named \"Elisabeth\" Foundation Alexis Carrel for thoracic and cardiovascular researches after Elisabeth Morrow, sister of Lindbergh\\'s wife Anne Morrow, who died from heart disease. It was in fact Lindbergh\\'s disappointment that contemporary medical technology could not provide an artificial heart pump which would allow for heart surgery on her that led to Lindbergh\\'s first contact with Carrel. == Alexis Carrel and Lourdes == In 1902 Alexis Carrel went from being a sceptic of the visions and miracles reported at Lourdes to being a believer in spiritual cures after experiencing a healing of Marie Bailly that he could not explain. Rev. Stanley Jaki Two Lourdes Miracles and a Nobel Laureate: What Really Happened? The Catholic journal Le nouvelliste reported that she named him as the prime witness of her cure. Alexis Carrel refused to discount a supernatural explanation and steadfastly reiterated his beliefs, even writing a book describing his experience, Alexis Carrel, The Voyage to Lourdes (New York, Harper & Row, 1939). though it was not published until four years after his death. This was a detriment to his career and reputation among his fellow doctors, and feeling he had no future in academic medicine in France, he immigrated to Canada with the intention of farming and raising cattle. After a brief period, he accepted an appointment at the University of Chicago and two years later at the Rockefeller Institute for the Study of Medicine. == Man, The Unknown (1935) == In 1935, Carrel published a book titled L\\'Homme, cet inconnu (Man, The Unknown), which became a best-seller. The book discussed \"the nature of society in light of discoveries in biology, physics, and medicine\". It contained his own social prescriptions, advocating, in part, that mankind could better itself by following the guidance of an elite group of intellectuals, and by implementing a regime of enforced eugenics. Carrel claimed the existence of a \"hereditary biological aristocracy\" and argued that \"deviant\" human types should be suppressed using techniques similar to those later employed by the Nazis. \"A euthanasia establishment, equipped with a suitable gas, would allow the humanitarian and economic disposal of those who have killed, committed armed robbery, kidnapped children, robbed the poor or seriously betrayed public confidence,\" Carrel wrote in L\\'Homme, cet Inconnu. \"Would the same system not be appropriate for lunatics who have committed criminal acts?\" he suggested. In the 1936 preface to the German edition of his book, Alexis Carrel added a praise to the eugenics policies of the Third Reich, writing that: (t)he German government has taken energetic measures against the propagation of the defective, the mentally diseased, and the criminal. The ideal solution would be the suppression of each of these individuals as soon as he has proven himself to be dangerous. Quoted in Andrés Horacio Reggiani. Alexis Carrel, the Unknown: Eugenics and Population Research under Vichy (French historical studies, 25:2 Spring 2002) , p. 339. Also quoted in French by Didier Daeninckx in Quand le négationnisme s’invite à l’université., on Amnistia.net website, , URL consulted on January 28, 2007  Carrel also wrote in his book that: (t)he conditioning of petty criminals with the whip, or some more scientific procedure, followed by a short stay in hospital, would probably suffice to insure order. Those who have murdered, robbed while armed with automatic pistol or machine gun, kidnapped children, despoiled the poor of their savings, misled the public in important matters, should be humanely and economically disposed of in small euthanasic institutions supplied with proper gasses. A similar treatment could be advantageously applied to the insane, guilty of criminal acts. Quoted in Szasz, Thomas. The Theology of Medicine New York: Syracuse University Press, 1977.  == The French Foundation for the Study of Human Problems == In 1937, Carrel joined Jean Coutrot’s Centre d’Etudes des Problèmes Humains - Coutrot’s aim was to develop what he called an \"economic humanism\" through \"collective thinking.\" In 1941, through connections to the cabinet of Vichy France president Philippe Pétain (specifically, French industrial physicians André Gros and Jacques Ménétrier) he went on to advocate for the creation of the Fondation Française pour l’Etude des Problèmes Humains (French Foundation for the Study of Human Problems) which was created by decree of the Vichy regime in 1941, and where he served as \\'regent\\'.  The foundation was at the origin of the October 11, 1946 law, enacted by the Provisional Government of the French Republic (GPRF), which institutionalized the field of occupational medicine. It worked on demographics (Robert Gessain, Paul Vincent, Jean Bourgeois-Pichat), on economics, (François Perroux), on nutrition (Jean Sutter), on habitation (Jean Merlet) and on the first opinion polls (Jean Stoetzel). \"The foundation was chartered as a public institution under the joint supervision of the ministries of finance and public health. It was given financial autonomy and a budget of forty million francs—roughly one franc per inhabitant—a true luxury considering the burdens imposed by the German Occupation on the nation’s resources. By way of comparison, the whole Centre National de la Recherche Scientifique (CNRS) was given a budget of fifty million francs.\" The Foundation made many positive accomplishments during its time. Yet it was also behind the origin of the December 16, 1942 Act inventing the \"prenuptial certificate\", which had to precede any marriage and was supposed, after a biological examination, to insure the \"good health\" of the spouses, in particular in regard to sexually transmitted diseases (STD) and \"life hygiene\" (sic). The institute also conceived the \"scholar book\" (\"livret scolaire\"), which could be used to record students\\' grades in the French secondary schools, and thus classify and select them according to scholastic performance. (Reggiani)  According to Gwen Terrenoire, writing in Eugenics in France (1913-1941) : a review of research findings, \"The foundation was a pluridisciplinary centre that employed around 300 researchers (mainly statisticians, psychologists, physicians) from the summer of 1942 to the end of the autumn of 1944. After the liberation of Paris, Carrel was suspended by the Minister of Health; he died in November 1944, but the Foundation itself was \"purged\", only to reappear in a short time as the Institut national d’études démographiques (INED) that is still active.\" Gwen Terrenoire, \"Eugenics in France (1913-1941): a review of research findings\", Joint Programmatic Commission UNESCO-ONG Science and Ethics, 2003) Although Carrel himself died before he could be tried for treason, most members of his team did move to the INED, which was led by famous demographist Alfred Sauvy, who coined the expression \"Third World\". Others joined Robert Debré\\'s \"Institut national d\\'hygiène\" (National Hygiene Institute), which later became the INSERM. == See also == *Nobel Prize in Physiology or Medicine *Lourdes *Charles Lindbergh *Eugenics ==Sources== * Carrel, Alexis. Man, The Unknown. New York and London: Harper and Brothers. 1935. * Szasz, Thomas. The Theology of Medicine New York: Syracuse University Press, 1977. * Feuerwerker, Elie. Alexis Carrel et l\\'eugénisme. Le Monde, 1er Juillet 1986. * Schneider, William. Quality and Quantity: The Quest for Biological Regeneration in Twentieth-Century France, Cambridge UP 1990. * Bonnafé, Lucien and Tort, Patrick. L\\'Homme, cet inconnu? Alexis Carrel, Jean-Marie le Pen et les chambres a gaz Editions Syllepse, 1996. Amazon.fr: L\\'Homme cet inconnu ? Alexis Carrel, Jean-Marie Le Pen et les Chambres à gaz: Lucien Bonnafé, Patrick Tort: Livres ISBN 2-907993-14-3 * David Zane Mairowitz. \"Fascism à la mode: in France, the far right presses for national purity\", Harper\\'s Magazine; 10/1/1997 * Reggiani, Andrés Horacio. Alexis Carrel, the Unknown: Eugenics and Population Research under Vichy French Historical Studies, Spring 2002; 25: pp. 331 - 356.  * Wallace, Max. The American Axis: Henry Ford, Charles Lindbergh, and the Rise of the Third Reich St. Martin\\'s Press, New York, 2003. * Berman, Paul. Terror and Liberalism W. W. Norton, 2003. * Walther, Rudolph. Die seltsamen Lehren des Doktor Carrel, DIE ZEIT 31.07.2003 Nr.32 Die seltsamen Lehren des Doktor Carrel: Wie ein katholischer Arzt aus Frankreich zum Vordenker der radikalen Islamisten wurde | Nachrichten auf ZEIT ONLINE  * Terrenoire, Gwen, CNRS. Eugenics in France (1913-1941) : a review of research findings Joint Programmatic Commission UNESCO-ONG Science and Ethics, March 24, 2003 Comité de Liaison ONG-UNESCO  * Reggiani, Andrés Horacio. God\\'s Eugenicist. Alexis Carrel and the Sociobiology of Decline. Berghahn Books, Oxford 2007. * Friedman, David M. The Immortalists: Charles Lindbergh, Dr. Alexis Carrel, and Their Daring Quest to Live Forever. HarperCollins, NY 2007. == References == ==External links== * Nobel Prize presentation speech to Dr. Carrel * Nobel Prize biography of Dr. Carrel * Research Foundation entitled to Alexis Carrel * Time Magazine, October 16, 1944 * Death of Alexis Carrel Time Magazine November 13, 1944\\n',\n"," '</text>',\n"," '</DOC>']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BvK8e6Jl18s","colab_type":"code","colab":{}},"source":["messages = text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZRS0xlp7W57","colab_type":"code","colab":{}},"source":["from xml.etree.cElementTree import iterparse\n","\n","# get an iterable and turn it into an iterator\n","context = iter(iterparse(\"20121202-wiki-en_000001.txt.xml\", events=(\"start\", \"end\")))\n","\n","# get the root element\n","event, root = next(context)\n","assert event == \"start\"\n","\n","for event, elem in context:\n","    if event == \"end\" and elem.tag == \"doc\":\n","       # ... process book elements ...\n","       root.clear()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEj2MDxQB_9M","colab_type":"code","colab":{}},"source":["import xml.etree.ElementTree as ET\n","context = ET.iterparse('20121202-wiki-en_000000.txt.xml', events=('end', ),parser = 'parser')\n","for event, elem in context:\n","    if elem.tag == 'doc':\n","        title = elem.find('title').text\n","        print(title)\n","        #filename = format(title)\n","        #with open(filename, 'wb') as f:\n","            #f.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n","            #f.write(ET.tostring(elem))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RurwL-VjGnPy","colab_type":"code","colab":{}},"source":["import xml.etree.ElementTree as ET\n","\n","context = ET.iterparse('20121202-wiki-en_000000.txt.xml', events=('end', ))\n","for event, elem in context:\n","  if elem.tag == 'doc':\n","    title = elem.find('title').text\n","    #filename = format(title + \".xml\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIpyD4EpbYXa","colab_type":"code","colab":{}},"source":["!pip install beautifulsoup4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zRFV_jzbZBS","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup as BS"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ro5Ajct5brUY","colab_type":"code","colab":{}},"source":["line='<City_State>PLAINSBORO, NJ 08536-1906</City_State>'\n","soup = BS(line)\n","print (soup.find('city_state').text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0fHCdk1crgj","colab_type":"code","colab":{}},"source":["text = \"\"\"<doc><title>Austin (disambiguation)</title>\n","<docno>wikipedia28</docno>\n","<text> Austin is the capital of the U.S. state of Texas. Austin may also refer to: ==Geographical locations== ===In the United States=== *Austin, Arkansas *Austin, Colorado *Austin, Chicago, Illinois *Austin, Indiana *Austin, Minnesota *Austin, Nevada *Austin, Oregon *Austin County, Texas (note that Austin, Texas, is located in Travis County, Texas) ===In Canada=== *Austin, Manitoba *Austin, Ontario *Austin, Quebec *Austin Island, Nunavut ===In Australia=== *Austin, Western Australia ===Elsewhere=== *Austin Station (disambiguation), various places *Austin (CTA Blue Line), on the Chicago Transit Authority's Blue Line *Austin (CTA Green Line), on the CTA's Green Line ==People== Austin is a given name and surname, an English language contraction of Augustine. It may refer to: ===Surname=== *Albert Austin (1881–1953), British/American actor *Alfred Austin (1835–1913), British poet *Bernard L. Austin (1902-1979), American admiral *Brett Austin (1959–1990), New Zealand swimmer *Bunny Austin (1906–2000), British tennis player *Charlie Austin (born 1989), English footballer *Charles Austin (born 1967), American athlete *Chase Austin (born 1989), American NASCAR racecar driver *Dallas Austin (born 1970), American musician *Denise Austin (born 1957), American fitness expert *Ennis Raymond Austin, American architect *Gene Austin (1900–1972), American singer *Henry Austin (architect) (1804–1891), American architect *Henry Austin (baseball) (1844–1904), American baseball player *Herbert Austin (1866–1941), British founder of the Austin Motor Company *Hubert Austin (1845–1915), British architect *Jake T. Austin (born 1994), American actor *John Austin (legal philosopher) (1790–1859),\n","</text>\n","<title>Seattle</title>\n","<docno>wikipedia23 </docno>\n","<text>asdjsbfsiafhasdiofj asiuDFH AO[ISFJ] </text>\n","<title>Raleigh</title>\n","<docno>wikipedia29</docno>\n","<text> ADSOAIFHAKLSFJAOIFH AIOFH AOIDH OIJ</text>\n","</doc>\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpuyrPs-p0vw","colab_type":"code","colab":{}},"source":["text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mXyOLbjp1s0","colab_type":"code","colab":{}},"source":["import re\n","comb = re.compile(r'<title>')\n","mo = comb.search(text)\n","print(mo.group())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5A7ngOQFMw2f","colab_type":"code","colab":{}},"source":["for line in text:\n","  if soup.find_all('title'):\n","    count = count +1\n","print(count)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcTB6Am-VXMX","colab_type":"code","colab":{}},"source":["soup = BS(text)\n","count = 0\n","soup.find_all('title')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4nE7IiHYsUg","colab_type":"code","colab":{}},"source":["import xml.etree.ElementTree as ET\n","context = ET.iterparse('20121202-wiki-en_000001.txt.xml', events=('end', ))\n","for event, elem in context:\n","    if elem.tag == 'doc':\n","        title2 = elem.find('title').text\n","        filename = 'ABC'\n","        with open(filename, 'wb') as f:\n","            #f.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n","            f.write(ET.tostring(elem))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PC0Pr38cdNs","colab_type":"code","colab":{}},"source":["import xml.etree.ElementTree as ET\n","\n","context = ET.iterparse('20121202-wiki-en_000001.txt.xml', events=('end', ))\n","for event, elem in context:\n","  print(event)\n","#  if elem.tag == 'doc':\n","#    print ('Yes')\n","#    title = elem.find('title').text\n","#   filename = format(title + \".xml\")\n","##    with open(filename, 'wb') as f:\n","#        f.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n","#        f.write(\"<root>\\n\")\n","#        f.write(ET.tostring(elem))\n","#        f.write(\"</root>\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pp0vHbztdM3X","colab_type":"code","colab":{}},"source":["# Creating and writing to a file in Python\n","\n","f= open(\"Paragraph_sample.txt\",\"w+\")\n","for line in paragraph2:\n","  f.write(line)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Xv4UYz5lwpJ","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVTBNJqZmt5q","colab_type":"code","colab":{}},"source":["messages = paragraph2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kaC0YOflyRE","colab_type":"code","colab":{}},"source":["import time\n","start_time = time.time()\n","\n","with tf.Session() as session:\n","  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n","  message_embeddings = session.run(embed(messages))\n","\n","  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    print(\"Message: {}\".format(messages[i]))\n","    print(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n","\n","\n","end_time = time.time()\n","print (end_time - start_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GK9ZVQqnHwK","colab_type":"code","colab":{}},"source":["with open('helloworld.txt', 'w+') as filehandle:\n","    filehandle.write(np.array(message_embeddings).tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2U41Orw1okIi","colab_type":"code","colab":{}},"source":["type(np.array(message_embeddings))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7rb3MvxpxDC","colab_type":"code","colab":{}},"source":["with open('', 'w+') as filehandle:\n","  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n","    filehandle.write(\"Message: {}\".format(messages[i]))\n","    filehandle.write(\"Embedding size: {}\".format(len(message_embedding)))\n","    message_embedding_snippet = \", \".join(\n","        (str(x) for x in message_embedding[:3]))\n","    filehandle.write(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AM7m2Cw6xV2B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}